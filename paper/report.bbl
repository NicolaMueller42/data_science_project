% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{qi2018and}
Y.~Qi, D.~S. Sachan, M.~Felix, S.~J. Padmanabhan, and G.~Neubig, ``When and why
  are pre-trained word embeddings useful for neural machine translation?''
  \emph{arXiv preprint arXiv:1804.06323}, 2018.

\bibitem{rezaeinia2019sentiment}
S.~M. Rezaeinia, R.~Rahmani, A.~Ghodsi, and H.~Veisi, ``Sentiment analysis
  based on improved pre-trained word embeddings,'' \emph{Expert Systems with
  Applications}, vol. 117, pp. 139--147, 2019.

\bibitem{ye2016word}
X.~Ye, H.~Shen, X.~Ma, R.~Bunescu, and C.~Liu, ``From word embeddings to
  document similarities for improved information retrieval in software
  engineering,'' in \emph{Proceedings of the 38th international conference on
  software engineering}, 2016, pp. 404--415.

\bibitem{mikolov2013efficient}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' \emph{arXiv preprint arXiv:1301.3781},
  2013.

\bibitem{pennington2014glove}
J.~Pennington, R.~Socher, and C.~D. Manning, ``Glove: Global vectors for word
  representation,'' in \emph{Proceedings of the 2014 conference on empirical
  methods in natural language processing (EMNLP)}, 2014, pp. 1532--1543.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{reimers2019sentence}
N.~Reimers and I.~Gurevych, ``Sentence-bert: Sentence embeddings using siamese
  bert-networks,'' \emph{arXiv preprint arXiv:1908.10084}, 2019.

\end{thebibliography}
